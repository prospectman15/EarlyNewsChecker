name: seed-and-train

on:
  workflow_dispatch:
    inputs:
      run_mode:
        description: "What to do"
        required: true
        default: "real_only"
        type: choice
        options: ["real_only","seed_only","both"]
      tickers:
        description: "Tickers CSV (used by backfill & seeding)"
        required: true
        default: "TSLA,AMD,PLTR,SMCI,COIN,BA,CRWD,AAPL,MSFT"
        type: string
      backfill_days:
        description: "Reddit backfill trailing days (<=7 recommended)"
        required: true
        default: "7"
        type: string
      subs:
        description: "Subreddits CSV"
        required: true
        default: "wallstreetbets,stocks,StockMarket"
        type: string
      window:
        description: "Cluster window (minutes)"
        required: true
        default: "60"
        type: string
      min_abs_ret_pct:
        description: "Label y=1 if |t+60m| >= this %"
        required: true
        default: "3"
        type: string
      gates:
        description: "Backfill gates"
        required: true
        default: "loose"
        type: choice
        options: ["loose","normal","strict"]
      seed_params:
        description: "Seeding params CSV: pos=0,neg=0,days=30,seed=73"
        required: true
        default: "pos=0,neg=0,days=30,seed=73"
        type: string

jobs:
  go:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - uses: actions/setup-python@v5
        with: { python-version: "3.11" }

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || true
          # Ensure these are present even if not in requirements.txt
          pip install requests yfinance pandas numpy scikit-learn --upgrade

      # Parse seeding params into env (POS/NEG/SDAYS/SSEED)
      - name: Parse seed_params
        id: parse
        env:
          SEED_PARAMS: ${{ inputs.seed_params }}
        run: |
          python - << 'PY' > parsed.env
          import os
          s = os.getenv("SEED_PARAMS","")
          d = dict(p.strip().split("=",1) for p in s.split(",") if "=" in p)
          def val(k, default): return d.get(k, default)
          print("POS="+val("pos","0"))
          print("NEG="+val("neg","0"))
          print("SDAYS="+val("days","30"))
          print("SSEED="+val("seed","73"))
          PY
          cat parsed.env >> $GITHUB_ENV

      # Map 'gates' input to numeric thresholds
      - name: Compute backfill gate vars
        id: gates
        run: |
          set -e
          GATES="${{ inputs.gates }}"
          if [ "$GATES" = "strict" ]; then
            echo "REQ_MATCHES=2"     >> $GITHUB_ENV
            echo "MIN_SCORE=120"     >> $GITHUB_ENV
            echo "MIN_COMMENTS=80"   >> $GITHUB_ENV
          elif [ "$GATES" = "normal" ]; then
            echo "REQ_MATCHES=2"     >> $GITHUB_ENV
            echo "MIN_SCORE=80"      >> $GITHUB_ENV
            echo "MIN_COMMENTS=40"   >> $GITHUB_ENV
          else
            echo "REQ_MATCHES=1"     >> $GITHUB_ENV
            echo "MIN_SCORE=0"       >> $GITHUB_ENV
            echo "MIN_COMMENTS=0"    >> $GITHUB_ENV
          fi

      # Write a robust Reddit backfill helper (no extra files in repo)
      - name: Write tools/backfill_reddit.py
        if: ${{ inputs.run_mode == 'real_only' || inputs.run_mode == 'both' }}
        run: |
          mkdir -p tools
          cat > tools/backfill_reddit.py << 'PY'
          #!/usr/bin/env python3
          import os, csv, re, time, argparse
          from datetime import datetime, timedelta, timezone
          from urllib.parse import urlparse
          import requests, yfinance as yf

          DATASET_DIR="dataset"
          SIGNALS=f"{DATASET_DIR}/signals.csv"
          LABELS=f"{DATASET_DIR}/labels.csv"
          OUTCOMES="outcomes.csv"
          DEBUG_COUNTS=f"{DATASET_DIR}/backfill_debug_counts.csv"

          CLUSTER_WINDOW_MIN=60
          REQUIRED_MATCHES=1
          SINGLE_POST_MIN_SCORE=0
          SINGLE_POST_MIN_COMMENTS=0
          MIN_ABS_RET_PCT=3.0
          LABEL_WINDOW_MIN=90
          MEDIA_ALLOW={"sec.gov","businesswire.com","prnewswire.com"}

          POS = ["upgrade","raises guidance","raise price target","price target","initiated coverage",
                 "beats","beat estimates","contract win","buyback","approval","fda","pt raise"]
          CRISIS = ["crash","emergency landing","grounded","evacuate","explosion","fire","smoke",
                    "engine failure","faa","ntsb","probe","recall","lawsuit","hack","breach","ransomware",
                    "outage","strike","walkout","ceo resign","halts"]
          EARN = [r"\bearnings\b", r"\beps\b","results","miss","beat","guide","guidance","revenue",
                  "margin","outlook","forecast","loss","profit"]

          def ensure_headers():
              os.makedirs(DATASET_DIR, exist_ok=True)
              if not os.path.exists(SIGNALS):
                  csv.writer(open(SIGNALS,"w",newline="",encoding="utf-8")).writerow([
                      "id","ticker","signal_iso",
                      "hits_in_window","score_max","score_sum","comments_max","comments_sum",
                      "has_positive","has_crisis","has_earnings","has_primary_source_link",
                      "titles_concat","links_concat",
                      "score_max","comments_max","hits"
                  ])
              if not os.path.exists(LABELS):
                  csv.writer(open(LABELS,"w",newline="",encoding="utf-8")).writerow(
                      ["ticker","iso_time","window_min","y","notes"])
              if not os.path.exists(OUTCOMES):
                  csv.writer(open(OUTCOMES,"w",newline="",encoding="utf-8")).writerow(
                      ["id","ticker","label","due_iso","alert_price","cur_price","ret_pct"])
              if not os.path.exists(DEBUG_COUNTS):
                  csv.writer(open(DEBUG_COUNTS,"w",newline="",encoding="utf-8")).writerow(
                      ["day","subreddit","ticker","posts_fetched","posts_kept","clusters_emitted"])

          def px_at(tkr, t_utc):
              # 1m bars available ~7 days back
              try:
                  df=yf.download(tickers=tkr, period="7d", interval="1m", progress=False, threads=False)
                  if df is None or df.empty: return None
                  idx=df.index.tz_convert(None) if getattr(df.index,"tz",None) else df.index
                  df=df.copy(); df.index=idx
                  tgt=t_utc.replace(second=0,microsecond=0).replace(tzinfo=None)
                  sub=df.loc[:tgt].tail(1)
                  return None if sub.empty else float(sub["Close"].iloc[-1])
              except: return None

          def rth_align(tu):
              et = tu.astimezone()
              if et.weekday()>=5:
                  days=7-et.weekday()
                  et=(et+timedelta(days=days)).replace(hour=9,minute=30,second=0,microsecond=0)
                  return et.astimezone(timezone.utc)
              o=et.replace(hour=9,minute=30,second=0,microsecond=0)
              c=et.replace(hour=16,minute=0,second=0,microsecond=0)
              if et<o: return o.astimezone(timezone.utc)
              if et>c:
                  nx=et+timedelta(days=1)
                  while nx.weekday()>=5: nx+=timedelta(days=1)
                  nx=nx.replace(hour=9,minute=30,second=0,microsecond=0)
                  return nx.astimezone(timezone.utc)
              return et.astimezone(timezone.utc)

          def t60_return(tkr, ats):
              p0=px_at(tkr, ats)
              if not p0: return None,None,None
              due=rth_align(ats + timedelta(minutes=60))
              p1=px_at(tkr, due)
              if not p1: return p0,None,None
              return p0,p1,(p1/p0 - 1.0)*100.0

          def pushshift_fetch(sub, q, after_ts, before_ts, size=250):
              url=( "https://api.pullpush.io/reddit/search/submission/"
                    f"?subreddit={sub}&q={requests.utils.quote(q)}&after={after_ts}&before={before_ts}"
                    f"&size={size}&sort=asc" )
              try:
                  r=requests.get(url,timeout=20,headers={"User-Agent":"early-news-backfill/1.0"})
                  if r.status_code!=200: return []
                  data=r.json().get("data",[])
                  out=[]
                  for d in data:
                      out.append({
                          "title": d.get("title","") or "",
                          "selftext": d.get("selftext","") or "",
                          "url": d.get("url") or "",
                          "created_utc": int(d.get("created_utc",0) or 0),
                          "num_comments": int(d.get("num_comments",0) or 0),
                          "score": int(d.get("score",0) or 0),
                          "subreddit": sub
                      })
                  return out
              except:
                  return []

          def compile_patterns(tkr):
              return [
                  re.compile(rf"\${re.escape(tkr)}\b", re.I),      # $TSLA
                  re.compile(rf"\b{re.escape(tkr)}\b", re.I),      # TSLA
              ]

          def post_mentions(post, pats):
              text = f"{post.get('title','')} {post.get('selftext','')}"
              return any(p.search(text) for p in pats)

          def has_primary(links):
              return any(any(dom in (l or "") for dom in MEDIA_ALLOW) for l in links)

          def contains_any(s, arr):
              t=(s or "").lower(); return any(x in t for x in arr)

          def contains_rx(s, rx):
              t=(s or "").lower()
              return any(re.search(p, t) for p in rx)

          def main():
              ap=argparse.ArgumentParser()
              ap.add_argument("--tickers", required=True)
              ap.add_argument("--days", type=int, default=7)
              ap.add_argument("--subs", default="wallstreetbets,stocks,StockMarket")
              ap.add_argument("--cluster_window", type=int, default=60)
              ap.add_argument("--required_matches", type=int, default=1)
              ap.add_argument("--single_post_min_score", type=int, default=0)
              ap.add_argument("--single_post_min_comments", type=int, default=0)
              ap.add_argument("--min_abs_ret_pct", type=float, default=3.0)
              a=ap.parse_args()

              global CLUSTER_WINDOW_MIN, REQUIRED_MATCHES, SINGLE_POST_MIN_SCORE, SINGLE_POST_MIN_COMMENTS, MIN_ABS_RET_PCT
              CLUSTER_WINDOW_MIN=a.cluster_window
              REQUIRED_MATCHES=a.required_matches
              SINGLE_POST_MIN_SCORE=a.single_post_min_score
              SINGLE_POST_MIN_COMMENTS=a.single_post_min_comments
              MIN_ABS_RET_PCT=a.min_abs_ret_pct

              tickers=[t.strip().upper() for t in a.tickers.split(",") if t.strip()]
              subs=[s.strip() for s in a.subs.split(",") if s.strip()]

              ensure_headers()

              end=datetime.now(timezone.utc)
              start=end - timedelta(days=a.days)

              with open(SIGNALS,"a",newline="",encoding="utf-8") as fs, \
                   open(LABELS,"a",newline="",encoding="utf-8") as fl, \
                   open(OUTCOMES,"a",newline="",encoding="utf-8") as fo, \
                   open(DEBUG_COUNTS,"a",newline="",encoding="utf-8") as fd:
                  ws, wl, wo, wdbg = csv.writer(fs), csv.writer(fl), csv.writer(fo), csv.writer(fd)

                  total_clusters=0
                  for d in range(a.days, 0, -1):
                      day_end=end - timedelta(days=d-1)
                      day_start=end - timedelta(days=d)
                      after_ts=int(day_start.timestamp()); before_ts=int(day_end.timestamp())
                      day_str=day_start.date().isoformat()

                      for sub in subs:
                          for tkr in tickers:
                              pats=compile_patterns(tkr)
                              q=f'"{tkr}" OR ${tkr} OR {tkr}'
                              posts = pushshift_fetch(sub, q, after_ts, before_ts)
                              fetched=len(posts)
                              kept=[p for p in posts if post_mentions(p, pats)]
                              kept.sort(key=lambda x:x["created_utc"])

                              # cluster
                              i=0; W=CLUSTER_WINDOW_MIN*60; seen=set(); emitted=0
                              while i<len(kept):
                                  anchor=kept[i]["created_utc"]; j=i; cluster=[]
                                  while j<len(kept) and kept[j]["created_utc"]<=anchor+W:
                                      cluster.append(kept[j]); j+=1
                                  i+=1

                                  hits=len(cluster)
                                  score_max=max([p.get("score",0) for p in cluster]) if cluster else 0
                                  comments_max=max([p.get("num_comments",0) for p in cluster]) if cluster else 0
                                  score_sum=sum([p.get("score",0) for p in cluster])
                                  comments_sum=sum([p.get("num_comments",0) for p in cluster])
                                  titles=[p["title"] for p in cluster[:4]]
                                  links=[p.get("url","") for p in cluster[:4]]
                                  has_pos=1 if any(contains_any(p["title"], POS) for p in cluster) else 0
                                  has_cri=1 if any(contains_any(p["title"], CRISIS) for p in cluster) else 0
                                  has_earn=1 if any(contains_rx(p["title"], EARN) for p in cluster) else 0
                                  has_pri=1 if has_primary(links) else 0

                                  cluster_ok = hits>=REQUIRED_MATCHES
                                  consensus_ok = (score_max>=SINGLE_POST_MIN_SCORE and comments_max>=SINGLE_POST_MIN_COMMENTS)
                                  if not (cluster_ok or consensus_ok): 
                                      continue

                                  bucket = anchor//1800
                                  key=(tkr,bucket)
                                  if key in seen: 
                                      continue
                                  seen.add(key)

                                  ts=datetime.fromtimestamp(anchor,tz=timezone.utc).isoformat()
                                  sid=f"BF-{tkr}-{anchor}"
                                  ws.writerow([sid,tkr,ts,hits,score_max,score_sum,comments_max,comments_sum,
                                               has_pos,has_cri,has_earn,has_pri," || ".join(titles)," || ".join(links),
                                               score_max,comments_max,hits])
                                  emitted += 1
                                  total_clusters += 1

                                  # label from t+60m
                                  ats=datetime.fromtimestamp(anchor,tz=timezone.utc)
                                  p0,p1,ret=t60_return(tkr,ats)
                                  if p0 is not None and p1 is not None and ret is not None:
                                      wo.writerow([sid,tkr,"t+60m",(ats+timedelta(minutes=60)).isoformat(),
                                                   f"{p0:.4f}",f"{p1:.4f}",f"{ret:.3f}"])
                                      y = 1 if abs(ret)>=MIN_ABS_RET_PCT else 0
                                      wl.writerow([tkr,ts,str(LABEL_WINDOW_MIN), str(y), f"backfill t+60m |Δ|={ret:.2f}%"])
                                  else:
                                      wl.writerow([tkr,ts,str(LABEL_WINDOW_MIN), "0", "backfill: price missing"])

                              wdbg.writerow([day_str, sub, tkr, fetched, len(kept), emitted])
                              print(f"[{day_str}] r/{sub} {tkr}: fetched={fetched} kept={len(kept)} clusters={emitted}")

                  print(f"Backfill done. total_clusters={total_clusters}")

          if __name__ == "__main__":
              main()
          PY
          chmod +x tools/backfill_reddit.py

      # Backfill Reddit chatter → signals + labels
      - name: Backfill Reddit chatter → signals + labels
        if: ${{ inputs.run_mode == 'real_only' || inputs.run_mode == 'both' }}
        run: |
          python tools/backfill_reddit.py \
            --tickers "${{ inputs.tickers }}" \
            --days "${{ inputs.backfill_days }}" \
            --subs "${{ inputs.subs }}" \
            --cluster_window "${{ inputs.window }}" \
            --required_matches "${{ env.REQ_MATCHES }}" \
            --single_post_min_score "${{ env.MIN_SCORE }}" \
            --single_post_min_comments "${{ env.MIN_COMMENTS }}" \
            --min_abs_ret_pct "${{ inputs.min_abs_ret_pct }}"

      # Inspect what changed (so you can tell at a glance)
      - name: Inspect dataset counts
        run: |
          echo "signals rows: $(wc -l < dataset/signals.csv 2>/dev/null || echo 0)"
          echo "labels rows:  $(wc -l < dataset/labels.csv 2>/dev/null || echo 0)"
          echo "outcomes rows:$(wc -l < outcomes.csv 2>/dev/null || echo 0)"
          echo "--- last 5 signals ---"; tail -n 5 dataset/signals.csv || true
          echo "--- last 5 outcomes ---"; tail -n 5 outcomes.csv || true
          echo "--- last 5 debug counts ---"; tail -n 5 dataset/backfill_debug_counts.csv || true

      # Synthetic seeding (optional)
      - name: Seed dataset (synthetic)
        if: ${{ inputs.run_mode == 'seed_only' || inputs.run_mode == 'both' }}
        run: |
          python tools/seed_dataset.py \
            --pos "${POS}" \
            --neg "${NEG}" \
            --days "${SDAYS}" \
            --tickers "${{ inputs.tickers }}" \
            --seed "${SSEED}"

      - name: Commit dataset
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add -f dataset/* outcomes.csv || true
          git commit -m "dataset: backfill+seed run (mode=${{ inputs.run_mode }}, gates=${{ inputs.gates }})" || echo "Nothing to commit"
          git push || true

      - name: Train model
        run: python train.py

      - name: Commit model
        run: |
          git add -f model/model.pkl model/metrics.json || true
          git commit -m "model: update after seed-and-train" || echo "Nothing to commit"
          git push || true

      - name: Show outputs
        run: |
          echo "=== dataset ==="; ls -al dataset || true
          echo "=== model ===";  ls -al model || true
